## 面经

- 为什么LSTM比RNN好

  - LSTM如何来避免梯度弥散和梯度爆炸？ - Towser的回答 - 知乎 https://www.zhihu.com/question/34878706/answer/665429718

- batch size怎么设置？

  - 并不是越大越好。小的batch size可以带来更多的噪声，防止陷入局部最小值（鞍点），但是训练时间长。大的batch size可以缩短训练时间（更好的并行）。
  - 怎么选取训练神经网络时的Batch size? - 夕小瑶的回答-知乎 https://www.zhihu.com/question/61607442/answer/204525634
  - 怎么选取训练神经网络时的Batch size? - Matt的回答 - 知乎 https://www.zhihu.com/question/61607442/answer/192204021
  - 如何设置合适的 batch 大小收获 4 倍加速 & 更好的泛化效果：https://www.leiphone.com/news/201911/XthpCSNTEk03RaPI.html
  - 英文原文：https://towardsdatascience.com/implementing-a-batch-size-finder-in-fastai-how-to-get-a-4x-speedup-with-better-generalization-813d686f6bdf

- learning rate 怎么设置？

  - pytorch中有多种学习率设置方式：
    - 有序调整：依一定规律有序进行调整，这一类是最常用的，分别是等间隔下降(Step)，按需设定下降间隔(MultiStep)，指数下降(Exponential)和CosineAnnealing。这四种方法的调整时机都是人为可控的，也是训练时常用到的。
    - 自适应调整：依训练状况伺机调整，这就是ReduceLROnPlateau方法。该法通过监测某一指标的变化情况，当该指标不再怎么变化的时候，就是调整学习率的时机，因而属于自适应的调整。
    - 自定义调整：Lambda。Lambda方法提供的调整策略十分灵活，我们可以为不同的层设定不同的学习率调整方法，这在fine-tune中十分有用，我们不仅可为不同的层设定不同的学习率，还可以为其设定不同的学习率调整策略，简直不能更棒！
  - fastai有一个工具可以根据lr-loss图，选取一个较好的leraning rate。
  - PyTorch 学习笔记（八）：PyTorch的六个学习率调整方法 - 余霆嵩的文章 - 知乎 https://zhuanlan.zhihu.com/p/69411064
  - [译]如何找到一个好的学习率(learning rate) - 陈志远的文章 - 知乎 https://zhuanlan.zhihu.com/p/50499794
  - 上面的英文原文：https://sgugger.github.io/how-do-you-find-a-good-learning-rate.html
  - 如何找到最优学习率 - Sherlock的文章 - 知乎 https://zhuanlan.zhihu.com/p/31424275
  - [机器之心]机器学习算法如何调参？一份神经网络学习速率设置指南 https://www.jiqizhixin.com/articles/nn-learning-rate

- 深度学习中batch size 和 learning rate有什么关系？
  $$
  \text { new_learningrate }=\text { old_learningrate } * \sqrt{\text {new_batchsize} / \text {old_batchsize}}
  $$

  - https://blog.csdn.net/qq_27261889/article/details/103120985 
  - https://arxiv.org/pdf/1404.5997.pdf 











# Plan of next week

- 


